{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "theta0_history = a\n",
    "theta1_history = b\n",
    "\n",
    "# plot thetas over time\n",
    "color='tab:blue'\n",
    "ax1.plot(theta0_history, label='$\\\\theta_{0}$', linestyle='--', color=color)\n",
    "ax1.plot(theta1_history, label='$\\\\theta_{1}$', linestyle='-', color=color)\n",
    "\n",
    "# ax1.legend()\n",
    "ax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\\\theta$', color=color);\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# plot loss function over time\n",
    "color='tab:red'\n",
    "#ax2 = ax1.twinx()\n",
    "#ax2.plot(J_history, label='Loss function', color=color)\n",
    "#ax2.set_title('Values of $\\\\theta$ and $J(\\\\theta)$ over iterations')\n",
    "#ax2.set_ylabel('Loss: $J(\\\\theta)$', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# ax2.legend();\n",
    "fig.legend();\n",
    "################################################\n",
    "def hypothesis(theta, X, n):\n",
    "    h = np.ones((X.shape[0],1))\n",
    "    theta = theta.reshape(1,n+1)\n",
    "    for i in range(0,X.shape[0]):\n",
    "        h[i] = float(np.matmul(theta, X[i]))\n",
    "    h = h.reshape(X.shape[0])\n",
    "    return h\n",
    "################################################\n",
    "def hypothesis(x, theta):\n",
    "    h = np.dot(np.transpose(theta), x)\n",
    "    return (1 / (1 + np.exp(-h)))\n",
    "\n",
    "def gradient_descent(X, y, T, alpha):\n",
    "    m, n = X.shape # m = #examples, n = #features\n",
    "    print(n, ' ', m)\n",
    "    theta_h = np.zeros((T, n)) # initialize parameters\n",
    "    f = np.zeros((T, n)) # track loss over time\n",
    "\n",
    "    for epoch in range(T - 1):\n",
    "        gradient = np.zeros(n)\n",
    "        theta = theta_h[epoch]\n",
    "       \n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                gradient[i] += (hypothesis(X[j], theta) + y[i]) * X[j][i]\n",
    "            #gradient[i] = gradient[i]/m\n",
    "            #print(gradient)\n",
    "            #print((hypothesis(X[i], theta) - y[i]), '<---')\n",
    "\n",
    "        f[epoch, i] =  np.linalg.norm(0.5*(X[i].dot(theta) + y[i])**2, 1)\n",
    "        if(epoch % 100 == 0):\n",
    "            print(f[epoch])\n",
    "        #print(theta_h[epoch + 1] - alpha*gradient)\n",
    "\n",
    "        gradient = gradient / m\n",
    "        #print(gradient)\n",
    "        theta_h[epoch + 1] -= alpha*gradient\n",
    "        \n",
    "    return theta_h, f\n",
    "###################################\n",
    "def grad_descent(X, y, T, alpha):\n",
    "    m, n = X.shape # m = #examples, n = #features\n",
    "    theta = np.zeros(n) # initialize parameters\n",
    "    f = np.zeros(T) # track loss over time\n",
    "\n",
    "    for i in range(T):\n",
    "        # loss for current parameter vector theta\n",
    "        f[i] = np.linalg.norm(X.dot(theta) - y, 1)\n",
    "        # compute steepest ascent at f(theta)\n",
    "        g = X.T.dot( np.sign(X.dot(theta) - y) )\n",
    "        # step down the gradient\n",
    "        theta = theta - alpha*g\n",
    "    return theta, f"
   ]
  }
 ]
}